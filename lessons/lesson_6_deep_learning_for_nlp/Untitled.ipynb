{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Embedding\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach\n",
    "Here's how we will solve the classification problem:\n",
    "\n",
    "1. convert all text samples in the dataset into sequences of word indices. A \"word index\" would simply be an integer ID for the word. We will only consider the top 20,000 most commonly occuring words in the dataset, and we will truncate the sequences to a maximum length of 1000 words.\n",
    "\n",
    "\n",
    "2. prepare an \"embedding matrix\" which will contain at index i the embedding vector for the word of index i in our word index.\n",
    "\n",
    "\n",
    "3. load this embedding matrix into a Keras Embedding layer, set to be frozen (its weights, the embedding vectors, will not be updated during training).\n",
    "\n",
    "\n",
    "4. build on top of it a 1D convolutional neural network, ending in a softmax output over our 20 categories.\n",
    "\n",
    "\n",
    "SOURCE: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_WORD = 'Good'\n",
    "EXAMPLE_SENTENCE = 'could have done better.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = [\n",
    "    'cat','Well done!',\n",
    "    'cat','Good work',\n",
    "    'cat Great effort',\n",
    "    'nice work',\n",
    "    'Excellent!',\n",
    "    'Weak',\n",
    "    'Poor effort!',\n",
    "    'not good',\n",
    "    'poor work',\n",
    "    'could have done better.'\n",
    "]\n",
    "\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_tf_api_names', '_tf_api_names_v1', 'char_level', 'document_count', 'filters', 'fit_on_sequences', 'fit_on_texts', 'get_config', 'index_docs', 'index_word', 'lower', 'num_words', 'oov_token', 'sequences_to_matrix', 'sequences_to_texts', 'sequences_to_texts_generator', 'split', 'texts_to_matrix', 'texts_to_sequences', 'texts_to_sequences_generator', 'to_json', 'word_counts', 'word_docs', 'word_index']\n"
     ]
    }
   ],
   "source": [
    "print(dir(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signature: t.fit_on_texts(texts)\n",
    "tokenizer.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example what the tokenizer has learned from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('cat', 3), ('well', 1), ('done', 2), ('good', 2), ('work', 3), ('great', 1), ('effort', 2), ('nice', 1), ('excellent', 1), ('weak', 1), ('poor', 2), ('not', 1), ('could', 1), ('have', 1), ('better', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'cat': 3, 'done': 2, 'well': 1, 'work': 3, 'good': 2, 'effort': 2, 'great': 1, 'nice': 1, 'excellent': 1, 'weak': 1, 'poor': 2, 'not': 1, 'could': 1, 'have': 1, 'better': 1})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cat': 1, 'work': 2, 'done': 3, 'good': 4, 'effort': 5, 'poor': 6, 'well': 7, 'great': 8, 'nice': 9, 'excellent': 10, 'weak': 11, 'not': 12, 'could': 13, 'have': 14, 'better': 15}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'cat', 2: 'work', 3: 'done', 4: 'good', 5: 'effort', 6: 'poor', 7: 'well', 8: 'great', 9: 'nice', 10: 'excellent', 11: 'weak', 12: 'not', 13: 'could', 14: 'have', 15: 'better'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### convert the sentences to a sequence of token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'Well done!']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1],\n",
       " [7, 3],\n",
       " [1],\n",
       " [4, 2],\n",
       " [1, 8, 5],\n",
       " [9, 2],\n",
       " [10],\n",
       " [11],\n",
       " [6, 5],\n",
       " [12, 4],\n",
       " [6, 2],\n",
       " [13, 14, 3, 15]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(docs)\n",
    "print(docs[0:2])\n",
    "encoded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pad the sequences to make the matrix size consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  0  0]\n",
      " [ 7  3  0  0]\n",
      " [ 1  0  0  0]\n",
      " [ 4  2  0  0]\n",
      " [ 1  8  5  0]\n",
      " [ 9  2  0  0]\n",
      " [10  0  0  0]\n",
      " [11  0  0  0]\n",
      " [ 6  5  0  0]\n",
      " [12  4  0  0]\n",
      " [ 6  2  0  0]\n",
      " [13 14  3 15]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OSDisk\n",
      " Volume Serial Number is 1E90-A1EF\n",
      "\n",
      " Directory of C:\\Users\\alsherman\\Desktop\\NLP\\nlp_practicum_cohort3\\lessons\n",
      "\n",
      "05/15/2019  10:51 PM    <DIR>          .\n",
      "05/15/2019  10:51 PM    <DIR>          ..\n",
      "05/13/2019  10:36 PM    <DIR>          .ipynb_checkpoints\n",
      "08/04/2014  01:15 PM       171,350,079 glove.6B.50d.txt\n",
      "05/13/2019  10:33 PM    <DIR>          lesson_0_configuration\n",
      "05/13/2019  10:33 PM    <DIR>          lesson_1_text_extraction\n",
      "05/13/2019  10:33 PM    <DIR>          lesson_2_text_preprocessing\n",
      "05/13/2019  10:33 PM    <DIR>          lesson_3_phrase_detection\n",
      "05/13/2019  10:33 PM    <DIR>          lesson_4_text_vectorization\n",
      "05/13/2019  10:33 PM    <DIR>          lesson_5_dimensionality_reduction\n",
      "05/13/2019  10:33 PM    <DIR>          lesson_6_word_embeddings\n",
      "05/13/2019  10:33 PM    <DIR>          lesson_7_text_similarity\n",
      "05/13/2019  10:33 PM    <DIR>          lesson_8_document_classification\n",
      "05/13/2019  10:33 PM    <DIR>          supplementary_material\n",
      "05/15/2019  10:51 PM            47,305 Untitled.ipynb\n",
      "               2 File(s)    171,397,384 bytes\n",
      "              13 Dir(s)  128,259,231,744 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glove Word Embeddings\n",
    "GLOVE_DIR = os.path.join('TODO', 'glove.6B.50d.txt') \n",
    "GLOVE_DIR = 'glove.6B.50d.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(GLOVE_DIR) as f:\n",
    "    for line in f:\n",
    "        print(line, '\\n')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: the\n",
      "\n",
      "EMBEDDING: [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open(GLOVE_DIR, 'rb') as f:\n",
    "\n",
    "    for line in f:\n",
    "        # separate the word from the embedding\n",
    "        values = line.split()\n",
    "        word = values[0].decode('utf-8')  # decode bytes to unicode\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "        # print the results\n",
    "        print(f'WORD: {word}\\n')\n",
    "        print(f'EMBEDDING: {embedding}')\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{b'the': array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
      "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
      "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
      "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
      "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
      "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
      "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
      "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
      "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
      "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open(GLOVE_DIR, 'rb') as f:\n",
    "\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0].decode('utf-8')\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "        # store the embeddings in a dict\n",
    "        embeddings_index[word] = embedding\n",
    "        \n",
    "        print(embeddings_index)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "Wall time: 6.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(GLOVE_DIR, 'rb') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0].decode('utf-8')\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "\n",
    "        # store the embeddings in a dict\n",
    "        embeddings_index[word] = embedding\n",
    "        \n",
    "print(f'Found {len(embeddings_index)} word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test out other words to see if a pre-trained embedding exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1643   ,  0.15722  , -0.55021  , -0.3303   ,  0.66463  ,\n",
       "       -0.1152   , -0.2261   , -0.23674  , -0.86119  ,  0.24319  ,\n",
       "        0.074499 ,  0.61081  ,  0.73683  , -0.35224  ,  0.61346  ,\n",
       "        0.0050975, -0.62538  , -0.0050458,  0.18392  , -0.12214  ,\n",
       "       -0.65973  , -0.30673  ,  0.35038  ,  0.75805  ,  1.0183   ,\n",
       "       -1.7424   , -1.4277   ,  0.38032  ,  0.37713  , -0.74941  ,\n",
       "        2.9401   , -0.8097   , -0.66901  ,  0.23123  , -0.073194 ,\n",
       "       -0.13624  ,  0.24424  , -1.0129   , -0.24919  , -0.06893  ,\n",
       "        0.70231  , -0.022177 , -0.64684  ,  0.59599  ,  0.027092 ,\n",
       "        0.11203  ,  0.61214  ,  0.74339  ,  0.23572  , -0.1369   ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"word\" # test out new words in the quotes (capitalization sensitive)\n",
    "\n",
    "embeddings_index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'cat', 2: 'work', 3: 'done', 4: 'good', 5: 'effort', 6: 'poor', 7: 'well', 8: 'great', 9: 'nice', 10: 'excellent', 11: 'weak', 12: 'not', 13: 'could', 14: 'have', 15: 'better'}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "[ 0.45281  -0.50108  -0.53714  -0.015697  0.22191   0.54602  -0.67301\n",
      " -0.6891    0.63493  -0.19726   0.33685   0.7735    0.90094   0.38488\n",
      "  0.38367   0.2657   -0.08057   0.61089  -1.2894   -0.22313  -0.61578\n",
      "  0.21697   0.35614   0.44499   0.60885  -1.1633   -1.1579    0.36118\n",
      "  0.10466  -0.78325   1.4352    0.18629  -0.26112   0.83275  -0.23123\n",
      "  0.32481   0.14485  -0.44552   0.33497  -0.95946  -0.097479  0.48138\n",
      " -0.43352   0.69455   0.91043  -0.28173   0.41637  -1.2609    0.71278\n",
      "  0.23782 ] \n",
      "\n",
      "work\n",
      "[ 5.1359e-01  1.9695e-01 -5.1944e-01 -8.6218e-01  1.5494e-02  1.0973e-01\n",
      " -8.0293e-01 -3.3361e-01 -1.6119e-04  1.0189e-02  4.6734e-02  4.6751e-01\n",
      " -4.7475e-01  1.1038e-01  3.9327e-01 -4.3652e-01  3.9984e-01  2.7109e-01\n",
      "  4.2650e-01 -6.0640e-01  8.1145e-01  4.5630e-01 -1.2726e-01 -2.2474e-01\n",
      "  6.4071e-01 -1.2767e+00 -7.2231e-01 -6.9590e-01  2.8045e-02 -2.3072e-01\n",
      "  3.7996e+00 -1.2625e-01 -4.7967e-01 -9.9972e-01 -2.1976e-01  5.0565e-01\n",
      "  2.5953e-02  8.0514e-01  1.9929e-01  2.8796e-01 -1.5915e-01 -3.0438e-01\n",
      "  1.6025e-01 -1.8290e-01 -3.8563e-02 -1.7619e-01  2.7041e-02  4.6842e-02\n",
      " -6.2897e-01  3.5726e-01] \n",
      "\n",
      "done\n",
      "[ 3.3076e-01 -4.3870e-01 -3.2163e-01 -4.9310e-01  1.0254e-01 -2.7421e-03\n",
      " -5.1720e-01  2.4336e-02 -1.2816e-01  1.4349e-01 -1.6691e-01  5.6121e-01\n",
      " -5.6241e-01 -4.0972e-02  7.5000e-01  2.3084e-01  5.3204e-01 -4.0973e-02\n",
      "  2.6892e-01 -6.9238e-01  2.7883e-01  3.7911e-01  5.6390e-01 -3.8150e-01\n",
      "  7.2132e-01 -1.3562e+00 -8.1717e-01 -5.4842e-02  5.7333e-01 -8.5489e-01\n",
      "  3.1889e+00  1.9918e-01 -4.2120e-01 -9.0427e-01 -1.9521e-01  3.0111e-01\n",
      "  4.6756e-01  8.2130e-01  6.0552e-02 -1.6143e-01 -2.6668e-01 -1.7660e-01\n",
      "  1.5820e-02  2.5528e-01 -9.6739e-02 -9.7282e-02 -8.4483e-02  3.3312e-01\n",
      " -2.2252e-01  7.4457e-01] \n",
      "\n",
      "good\n",
      "[-3.5586e-01  5.2130e-01 -6.1070e-01 -3.0131e-01  9.4862e-01 -3.1539e-01\n",
      " -5.9831e-01  1.2188e-01 -3.1943e-02  5.5695e-01 -1.0621e-01  6.3399e-01\n",
      " -4.7340e-01 -7.5895e-02  3.8247e-01  8.1569e-02  8.2214e-01  2.2220e-01\n",
      " -8.3764e-03 -7.6620e-01 -5.6253e-01  6.1759e-01  2.0292e-01 -4.8598e-02\n",
      "  8.7815e-01 -1.6549e+00 -7.7418e-01  1.5435e-01  9.4823e-01 -3.9520e-01\n",
      "  3.7302e+00  8.2855e-01 -1.4104e-01  1.6395e-02  2.1115e-01 -3.6085e-02\n",
      " -1.5587e-01  8.6583e-01  2.6309e-01 -7.1015e-01 -3.6770e-02  1.8282e-03\n",
      " -1.7704e-01  2.7032e-01  1.1026e-01  1.4133e-01 -5.7322e-02  2.7207e-01\n",
      "  3.1305e-01  9.2771e-01] \n",
      "\n",
      "effort\n",
      "[ 0.70902  -0.60861   0.26447  -0.54556  -0.080056  0.15464  -0.53953\n",
      "  0.31852   0.74595   0.45002  -0.67944  -0.093416 -0.62557   0.16601\n",
      " -0.34859  -0.53711   1.0038   -0.54608   0.14791  -0.37893   0.32426\n",
      "  0.047782 -0.71429  -1.0033    0.20843  -1.7756    0.12468  -0.61561\n",
      "  0.49403  -0.068333  3.1969    0.50705  -1.0984   -0.66999  -0.42717\n",
      "  0.33314  -0.5241    0.20927  -0.26731  -0.90293  -0.47239  -0.21518\n",
      " -0.16108  -0.56691  -0.01505  -0.028093  0.19612   0.62921  -0.23875\n",
      "  0.1726  ] \n",
      "\n",
      "poor\n",
      "[-0.53819  -0.40788  -0.1963   -0.80821   0.077145  0.025785 -0.50482\n",
      " -0.068887  0.13628   0.28619   0.08215  -0.26886   0.3935   -0.94845\n",
      "  0.21612  -0.19433   0.17588  -0.28373   0.53496  -0.26135  -0.54512\n",
      "  0.49143  -0.32752  -0.040662  0.41689  -1.0261   -0.056752 -0.43134\n",
      "  0.50692   0.69811   3.5451    0.79069   0.75455  -0.42825   0.075692\n",
      "  0.35466  -0.38997  -0.021445  0.55851  -0.56601  -0.79644   0.038788\n",
      "  0.79469   0.32506   0.20975   0.20248  -0.63181   0.05218   1.0731\n",
      "  0.035629] \n",
      "\n",
      "well\n",
      "[ 2.7691e-01  2.8745e-01 -2.9935e-01 -1.9964e-01  1.2956e-01  1.5555e-01\n",
      " -6.4522e-01 -3.4090e-01 -1.1833e-01  1.5798e-01  1.3969e-01  2.4872e-01\n",
      " -1.5901e-01 -3.3439e-02  1.1895e-01  7.6535e-02  4.5263e-01  2.6494e-01\n",
      " -1.9157e-01 -5.6768e-01  2.9286e-02  2.1745e-01  4.3406e-01  1.4981e-01\n",
      "  7.5774e-02 -1.4453e+00 -5.8394e-01 -4.6063e-02  6.6214e-02 -2.6417e-01\n",
      "  3.9650e+00  2.5196e-01  2.4855e-01 -5.0524e-01  2.5806e-01  2.8683e-01\n",
      " -1.7994e-01  6.2885e-01 -1.2040e-01 -4.2143e-02 -4.4911e-02  1.8561e-01\n",
      "  1.6266e-01 -2.6127e-03  1.3083e-01  2.0179e-01 -2.9667e-01 -9.4820e-02\n",
      " -2.1250e-01  2.2074e-02] \n",
      "\n",
      "great\n",
      "[-0.026567  1.3357   -1.028    -0.3729    0.52012  -0.12699  -0.35433\n",
      "  0.37824  -0.29716   0.093894 -0.034122  0.92961  -0.14023  -0.63299\n",
      "  0.020801 -0.21533   0.96923   0.47654  -1.0039   -0.24013  -0.36325\n",
      " -0.004757 -0.5148   -0.4626    1.2447   -1.8316   -1.5581   -0.37465\n",
      "  0.53362   0.20883   3.2209    0.64549   0.37438  -0.17657  -0.024164\n",
      "  0.33786  -0.419     0.40081  -0.11449   0.051232 -0.15205   0.29855\n",
      " -0.44052   0.11089  -0.24633   0.66251  -0.26949  -0.49658  -0.41618\n",
      " -0.2549  ] \n",
      "\n",
      "nice\n",
      "[ 0.20189   0.80606  -1.1281   -0.59593   0.52756  -0.4769   -0.5264\n",
      "  0.14526  -0.86087   0.56199  -0.43708  -0.16586  -0.23328  -0.21726\n",
      "  0.52114   0.062307  0.55115  -0.18002  -0.32983  -0.94434  -0.62019\n",
      "  0.78764  -0.36133   0.6858    0.3791   -0.87744  -0.76792   1.2885\n",
      "  1.142    -0.73489   2.3932    1.0967   -0.48686   0.5284    0.3927\n",
      " -0.056427  0.29632   1.0798    0.45157  -0.98115   1.0037    0.15634\n",
      "  0.022584 -0.14832  -0.092933  0.33691   0.42171  -0.21642   1.0139\n",
      "  1.0535  ] \n",
      "\n",
      "excellent\n",
      "[-0.40431   0.78002  -0.67538  -0.097149  0.54242  -0.51283  -0.47758\n",
      " -0.11429   0.55194   0.4953    0.25681   0.41516   0.38223   0.033936\n",
      " -0.51981  -0.5752    0.50975   0.44029  -0.16885  -0.75687  -0.36747\n",
      "  0.37269  -0.0867   -0.31057   0.7034   -0.46849  -0.47845   0.15794\n",
      "  0.23969   0.16104   2.7632    0.33381   0.47953  -0.3731    0.71095\n",
      "  0.6879    0.067399  1.4177   -0.3563   -0.48159   0.53281  -0.074593\n",
      "  0.016028  0.023371 -0.02953  -0.12463   0.12501   0.72255   0.4759\n",
      "  0.79514 ] \n",
      "\n",
      "weak\n",
      "[-0.26241  -1.1103    0.50271  -0.43052   0.37468  -0.3055    0.36708\n",
      "  0.25938  -0.16993   0.54245   0.63919   0.11347  -0.3919    0.31521\n",
      " -0.42901   0.49977  -0.2376   -0.79307   0.34494  -0.47877  -0.51945\n",
      " -0.50665   0.057701 -0.31797  -0.080134 -1.0289   -0.1507    0.50944\n",
      "  0.60715   1.3049    3.2575    0.11849   1.5057   -0.36649  -0.17726\n",
      " -0.20931  -0.59527  -0.025889 -0.2965   -1.1387   -0.52999   0.067286\n",
      "  0.094954  0.049722  0.51323  -0.11194  -0.007111  0.23775   0.68874\n",
      "  0.13873 ] \n",
      "\n",
      "not\n",
      "[ 5.5025e-01 -2.4942e-01 -9.3860e-04 -2.6400e-01  5.9320e-01  2.7950e-01\n",
      " -2.5666e-01  9.3076e-02 -3.6288e-01  9.0776e-02  2.8409e-01  7.1337e-01\n",
      " -4.7510e-01 -2.4413e-01  8.8424e-01  8.9109e-01  4.3009e-01 -2.7330e-01\n",
      "  1.1276e-01 -8.1665e-01 -4.1272e-01  1.7754e-01  6.1942e-01  1.0466e-01\n",
      "  3.3327e-01 -2.3125e+00 -5.2371e-01 -2.1898e-02  5.3801e-01 -5.0615e-01\n",
      "  3.8683e+00  1.6642e-01 -7.1981e-01 -7.4728e-01  1.1631e-01 -3.7585e-01\n",
      "  5.5520e-01  1.2675e-01 -2.2642e-01 -1.0175e-01 -3.5455e-01  1.2348e-01\n",
      "  1.6532e-01  7.0420e-01 -8.0231e-02 -6.8406e-02 -6.7626e-01  3.3763e-01\n",
      "  5.0139e-02  3.3465e-01] \n",
      "\n",
      "could\n",
      "[ 0.90754   -0.38322    0.67648   -0.20222    0.15156    0.13627\n",
      " -0.48813    0.48223   -0.095715   0.18306    0.27007    0.41415\n",
      " -0.48933   -0.0076005  0.79662    1.0989     0.53802   -0.54468\n",
      " -0.16063   -0.98348   -0.19188   -0.2144     0.19959   -0.31341\n",
      "  0.24101   -2.2662    -0.25926   -0.10898    0.66177   -0.48104\n",
      "  3.6298     0.45397   -0.64484   -0.52244    0.042922  -0.16605\n",
      "  0.097102   0.044836   0.20389   -0.46322   -0.46434    0.32394\n",
      "  0.25984    0.40849    0.20351    0.058722  -0.16408    0.20672\n",
      " -0.1844     0.071147 ] \n",
      "\n",
      "have\n",
      "[ 0.94911   -0.34968    0.48125   -0.19306   -0.0088384  0.28182\n",
      " -0.9613    -0.13581   -0.43083   -0.092933   0.15689    0.059585\n",
      " -0.49635   -0.17414    0.75661    0.4921     0.21773   -0.22778\n",
      " -0.13686   -0.90589   -0.48781    0.19919    0.91447   -0.16203\n",
      " -0.20645   -1.7312    -0.47622   -0.04854   -0.14027   -0.45828\n",
      "  4.0326     0.6052     0.10448   -0.7361     0.2485    -0.033461\n",
      " -0.13395    0.052782  -0.27268    0.079825  -0.80127    0.30831\n",
      "  0.43567    0.88747    0.29816   -0.02465   -0.95075    0.36233\n",
      " -0.72512   -0.6089   ] \n",
      "\n",
      "better\n",
      "[-0.1209   -0.16821   0.24099  -0.30287   0.43578  -0.38367  -0.55203\n",
      " -0.28681  -0.10092   0.47769   0.28969   0.29549  -0.44074  -0.13494\n",
      "  0.26022   0.45371   0.53749   0.06122   0.24366  -0.87761  -0.56241\n",
      "  0.24927   0.17941  -0.016943  0.57974  -1.3546   -0.53161  -0.26451\n",
      "  0.68211  -0.30482   3.7943    0.96326  -0.073896 -0.41032   0.24478\n",
      " -0.14579  -0.086789  0.995     0.049928 -0.60302  -0.36585  -0.10114\n",
      "  0.40423   0.25951   0.087927  0.06196   0.075266  0.12755   0.066461\n",
      "  1.1163  ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate through all the words in the sentence\n",
    "for ind, word in tokenizer.index_word.items():    \n",
    "\n",
    "    # get the embedding for the word\n",
    "    embedding_vector = embeddings_index.get(word, 'no embedding')\n",
    "\n",
    "    # view the word and embedding\n",
    "    print(word)\n",
    "    print(embedding_vector, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 50))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():    \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    # add each word in the embedding_matrix in the slot for the tokenizer's word id\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 4.52809989e-01 -5.01079977e-01 -5.37140012e-01 -1.56970005e-02\n",
      "   2.21910000e-01  5.46019971e-01 -6.73009992e-01 -6.89100027e-01\n",
      "   6.34930015e-01 -1.97260007e-01  3.36849988e-01  7.73500025e-01\n",
      "   9.00940001e-01  3.84880006e-01  3.83670002e-01  2.65700012e-01\n",
      "  -8.05699974e-02  6.10889971e-01 -1.28939998e+00 -2.23130003e-01\n",
      "  -6.15779996e-01  2.16969997e-01  3.56139988e-01  4.44990009e-01\n",
      "   6.08850002e-01 -1.16330004e+00 -1.15789998e+00  3.61180007e-01\n",
      "   1.04659997e-01 -7.83249974e-01  1.43519998e+00  1.86289996e-01\n",
      "  -2.61119992e-01  8.32750022e-01 -2.31230006e-01  3.24809998e-01\n",
      "   1.44850001e-01 -4.45520014e-01  3.34969997e-01 -9.59460020e-01\n",
      "  -9.74790007e-02  4.81379986e-01 -4.33519989e-01  6.94549978e-01\n",
      "   9.10430014e-01 -2.81729996e-01  4.16370004e-01 -1.26090002e+00\n",
      "   7.12779999e-01  2.37819999e-01]\n",
      " [ 5.13589978e-01  1.96950004e-01 -5.19439995e-01 -8.62179995e-01\n",
      "   1.54940002e-02  1.09729998e-01 -8.02929997e-01 -3.33609998e-01\n",
      "  -1.61189993e-04  1.01889996e-02  4.67340015e-02  4.67510015e-01\n",
      "  -4.74750012e-01  1.10380001e-01  3.93269986e-01 -4.36520010e-01\n",
      "   3.99839997e-01  2.71090001e-01  4.26499993e-01 -6.06400013e-01\n",
      "   8.11450005e-01  4.56299990e-01 -1.27260000e-01 -2.24739999e-01\n",
      "   6.40709996e-01 -1.27670002e+00 -7.22310007e-01 -6.95900023e-01\n",
      "   2.80450005e-02 -2.30719998e-01  3.79959989e+00 -1.26249999e-01\n",
      "  -4.79669988e-01 -9.99719977e-01 -2.19760001e-01  5.05649984e-01\n",
      "   2.59530004e-02  8.05140018e-01  1.99290007e-01  2.87959993e-01\n",
      "  -1.59150004e-01 -3.04380000e-01  1.60249993e-01 -1.82899997e-01\n",
      "  -3.85629982e-02 -1.76190004e-01  2.70409994e-02  4.68420014e-02\n",
      "  -6.28970027e-01  3.57259989e-01]\n",
      " [ 3.30760002e-01 -4.38699991e-01 -3.21630001e-01 -4.93099988e-01\n",
      "   1.02540001e-01 -2.74210004e-03 -5.17199993e-01  2.43360009e-02\n",
      "  -1.28160000e-01  1.43490002e-01 -1.66909993e-01  5.61209977e-01\n",
      "  -5.62409997e-01 -4.09720019e-02  7.50000000e-01  2.30839998e-01\n",
      "   5.32040000e-01 -4.09730002e-02  2.68920004e-01 -6.92380011e-01\n",
      "   2.78829992e-01  3.79110008e-01  5.63899994e-01 -3.81500006e-01\n",
      "   7.21319973e-01 -1.35619998e+00 -8.17170024e-01 -5.48419990e-02\n",
      "   5.73329985e-01 -8.54889989e-01  3.18889999e+00  1.99180007e-01\n",
      "  -4.21200007e-01 -9.04269993e-01 -1.95209995e-01  3.01109999e-01\n",
      "   4.67559993e-01  8.21300030e-01  6.05520010e-02 -1.61430001e-01\n",
      "  -2.66680002e-01 -1.76599994e-01  1.58200003e-02  2.55279988e-01\n",
      "  -9.67390016e-02 -9.72819999e-02 -8.44829977e-02  3.33119988e-01\n",
      "  -2.22519994e-01  7.44570017e-01]\n",
      " [-3.55859995e-01  5.21300018e-01 -6.10700011e-01 -3.01310003e-01\n",
      "   9.48620021e-01 -3.15389991e-01 -5.98309994e-01  1.21880002e-01\n",
      "  -3.19430009e-02  5.56949973e-01 -1.06210001e-01  6.33989990e-01\n",
      "  -4.73399997e-01 -7.58949965e-02  3.82470012e-01  8.15690011e-02\n",
      "   8.22139978e-01  2.22200006e-01 -8.37639999e-03 -7.66200006e-01\n",
      "  -5.62529981e-01  6.17590010e-01  2.02920005e-01 -4.85979989e-02\n",
      "   8.78149986e-01 -1.65489995e+00 -7.74179995e-01  1.54349998e-01\n",
      "   9.48230028e-01 -3.95200014e-01  3.73020005e+00  8.28549981e-01\n",
      "  -1.41039997e-01  1.63950007e-02  2.11150005e-01 -3.60849984e-02\n",
      "  -1.55870005e-01  8.65830004e-01  2.63090014e-01 -7.10150003e-01\n",
      "  -3.67700011e-02  1.82819995e-03 -1.77039996e-01  2.70319998e-01\n",
      "   1.10260002e-01  1.41330004e-01 -5.73219992e-02  2.72069991e-01\n",
      "   3.13050002e-01  9.27709997e-01]\n",
      " [ 7.09020019e-01 -6.08609974e-01  2.64470011e-01 -5.45560002e-01\n",
      "  -8.00559968e-02  1.54640004e-01 -5.39529979e-01  3.18520010e-01\n",
      "   7.45949984e-01  4.50019985e-01 -6.79440022e-01 -9.34159979e-02\n",
      "  -6.25569999e-01  1.66010007e-01 -3.48589987e-01 -5.37109971e-01\n",
      "   1.00380003e+00 -5.46079993e-01  1.47909999e-01 -3.78930002e-01\n",
      "   3.24259996e-01  4.77820002e-02 -7.14290023e-01 -1.00329995e+00\n",
      "   2.08430007e-01 -1.77559996e+00  1.24679998e-01 -6.15610003e-01\n",
      "   4.94029999e-01 -6.83329999e-02  3.19689989e+00  5.07049978e-01\n",
      "  -1.09840000e+00 -6.69990003e-01 -4.27170008e-01  3.33139986e-01\n",
      "  -5.24100006e-01  2.09270000e-01 -2.67309994e-01 -9.02930021e-01\n",
      "  -4.72389996e-01 -2.15179995e-01 -1.61080003e-01 -5.66910028e-01\n",
      "  -1.50499996e-02 -2.80930009e-02  1.96119994e-01  6.29209995e-01\n",
      "  -2.38749996e-01  1.72600001e-01]\n",
      " [-5.38190007e-01 -4.07880008e-01 -1.96300000e-01 -8.08210015e-01\n",
      "   7.71450028e-02  2.57849991e-02 -5.04819989e-01 -6.88870028e-02\n",
      "   1.36280000e-01  2.86190003e-01  8.21499974e-02 -2.68860012e-01\n",
      "   3.93500000e-01 -9.48450029e-01  2.16120005e-01 -1.94330007e-01\n",
      "   1.75880000e-01 -2.83730000e-01  5.34959972e-01 -2.61350006e-01\n",
      "  -5.45120001e-01  4.91430014e-01 -3.27520013e-01 -4.06620018e-02\n",
      "   4.16889995e-01 -1.02610004e+00 -5.67520000e-02 -4.31340009e-01\n",
      "   5.06919980e-01  6.98109984e-01  3.54509997e+00  7.90690005e-01\n",
      "   7.54549980e-01 -4.28250015e-01  7.56919980e-02  3.54660004e-01\n",
      "  -3.89970005e-01 -2.14450005e-02  5.58510005e-01 -5.66009998e-01\n",
      "  -7.96440005e-01  3.87879983e-02  7.94690013e-01  3.25060010e-01\n",
      "   2.09749997e-01  2.02480003e-01 -6.31810009e-01  5.21799996e-02\n",
      "   1.07309997e+00  3.56290005e-02]\n",
      " [ 2.76910007e-01  2.87449986e-01 -2.99349993e-01 -1.99640006e-01\n",
      "   1.29559994e-01  1.55550003e-01 -6.45219982e-01 -3.40900004e-01\n",
      "  -1.18330002e-01  1.57979995e-01  1.39689997e-01  2.48720005e-01\n",
      "  -1.59009993e-01 -3.34389992e-02  1.18950002e-01  7.65350014e-02\n",
      "   4.52630013e-01  2.64939994e-01 -1.91569999e-01 -5.67680001e-01\n",
      "   2.92860009e-02  2.17449993e-01  4.34060007e-01  1.49810001e-01\n",
      "   7.57739991e-02 -1.44529998e+00 -5.83940029e-01 -4.60629985e-02\n",
      "   6.62140027e-02 -2.64169991e-01  3.96499991e+00  2.51960009e-01\n",
      "   2.48549998e-01 -5.05240023e-01  2.58060008e-01  2.86830008e-01\n",
      "  -1.79940000e-01  6.28849983e-01 -1.20399997e-01 -4.21429984e-02\n",
      "  -4.49110009e-02  1.85609996e-01  1.62660003e-01 -2.61269999e-03\n",
      "   1.30830005e-01  2.01790005e-01 -2.96669990e-01 -9.48200002e-02\n",
      "  -2.12500006e-01  2.20740009e-02]\n",
      " [-2.65670009e-02  1.33570004e+00 -1.02800000e+00 -3.72900009e-01\n",
      "   5.20120025e-01 -1.26990005e-01 -3.54330003e-01  3.78239989e-01\n",
      "  -2.97160000e-01  9.38939974e-02 -3.41220014e-02  9.29610014e-01\n",
      "  -1.40230000e-01 -6.32990003e-01  2.08010003e-02 -2.15330005e-01\n",
      "   9.69229996e-01  4.76539999e-01 -1.00390005e+00 -2.40130007e-01\n",
      "  -3.63249987e-01 -4.75700013e-03 -5.14800012e-01 -4.62599993e-01\n",
      "   1.24469995e+00 -1.83159995e+00 -1.55809999e+00 -3.74650002e-01\n",
      "   5.33620000e-01  2.08829999e-01  3.22090006e+00  6.45489991e-01\n",
      "   3.74379992e-01 -1.76569998e-01 -2.41640005e-02  3.37859988e-01\n",
      "  -4.19000000e-01  4.00810003e-01 -1.14490002e-01  5.12319990e-02\n",
      "  -1.52050003e-01  2.98550010e-01 -4.40519989e-01  1.10890001e-01\n",
      "  -2.46329993e-01  6.62509978e-01 -2.69490004e-01 -4.96580005e-01\n",
      "  -4.16180015e-01 -2.54900008e-01]\n",
      " [ 2.01890007e-01  8.06060016e-01 -1.12810004e+00 -5.95929980e-01\n",
      "   5.27559996e-01 -4.76900011e-01 -5.26400030e-01  1.45260006e-01\n",
      "  -8.60870004e-01  5.61990023e-01 -4.37079996e-01 -1.65859997e-01\n",
      "  -2.33280003e-01 -2.17260003e-01  5.21139979e-01  6.23070002e-02\n",
      "   5.51150024e-01 -1.80020005e-01 -3.29829991e-01 -9.44339991e-01\n",
      "  -6.20190024e-01  7.87639976e-01 -3.61330003e-01  6.85800016e-01\n",
      "   3.79099995e-01 -8.77439976e-01 -7.67920017e-01  1.28849995e+00\n",
      "   1.14199996e+00 -7.34889984e-01  2.39319992e+00  1.09669995e+00\n",
      "  -4.86860007e-01  5.28400004e-01  3.92699987e-01 -5.64269982e-02\n",
      "   2.96319991e-01  1.07980001e+00  4.51570004e-01 -9.81149971e-01\n",
      "   1.00370002e+00  1.56340003e-01  2.25840006e-02 -1.48320004e-01\n",
      "  -9.29329991e-02  3.36910009e-01  4.21710014e-01 -2.16419995e-01\n",
      "   1.01390004e+00  1.05350006e+00]]\n"
     ]
    }
   ],
   "source": [
    "# view the embedding look up table\n",
    "print(embedding_matrix[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### View an example of looking up a word by the tokenizer id in the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.5586e-01,  5.2130e-01, -6.1070e-01, -3.0131e-01,  9.4862e-01,\n",
       "       -3.1539e-01, -5.9831e-01,  1.2188e-01, -3.1943e-02,  5.5695e-01,\n",
       "       -1.0621e-01,  6.3399e-01, -4.7340e-01, -7.5895e-02,  3.8247e-01,\n",
       "        8.1569e-02,  8.2214e-01,  2.2220e-01, -8.3764e-03, -7.6620e-01,\n",
       "       -5.6253e-01,  6.1759e-01,  2.0292e-01, -4.8598e-02,  8.7815e-01,\n",
       "       -1.6549e+00, -7.7418e-01,  1.5435e-01,  9.4823e-01, -3.9520e-01,\n",
       "        3.7302e+00,  8.2855e-01, -1.4104e-01,  1.6395e-02,  2.1115e-01,\n",
       "       -3.6085e-02, -1.5587e-01,  8.6583e-01,  2.6309e-01, -7.1015e-01,\n",
       "       -3.6770e-02,  1.8282e-03, -1.7704e-01,  2.7032e-01,  1.1026e-01,\n",
       "        1.4133e-01, -5.7322e-02,  2.7207e-01,  3.1305e-01,  9.2771e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4]]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.55859995e-01  5.21300018e-01 -6.10700011e-01 -3.01310003e-01\n",
      "  9.48620021e-01 -3.15389991e-01 -5.98309994e-01  1.21880002e-01\n",
      " -3.19430009e-02  5.56949973e-01 -1.06210001e-01  6.33989990e-01\n",
      " -4.73399997e-01 -7.58949965e-02  3.82470012e-01  8.15690011e-02\n",
      "  8.22139978e-01  2.22200006e-01 -8.37639999e-03 -7.66200006e-01\n",
      " -5.62529981e-01  6.17590010e-01  2.02920005e-01 -4.85979989e-02\n",
      "  8.78149986e-01 -1.65489995e+00 -7.74179995e-01  1.54349998e-01\n",
      "  9.48230028e-01 -3.95200014e-01  3.73020005e+00  8.28549981e-01\n",
      " -1.41039997e-01  1.63950007e-02  2.11150005e-01 -3.60849984e-02\n",
      " -1.55870005e-01  8.65830004e-01  2.63090014e-01 -7.10150003e-01\n",
      " -3.67700011e-02  1.82819995e-03 -1.77039996e-01  2.70319998e-01\n",
      "  1.10260002e-01  1.41330004e-01 -5.73219992e-02  2.72069991e-01\n",
      "  3.13050002e-01  9.27709997e-01]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create embeddings for an entire sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'could have done better.'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXAMPLE_SENTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 14, 3, 15]]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_ids = tokenizer.texts_to_sequences([EXAMPLE_SENTENCE])\n",
    "sentence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>could</th>\n",
       "      <td>0.90754</td>\n",
       "      <td>-0.38322</td>\n",
       "      <td>0.67648</td>\n",
       "      <td>-0.20222</td>\n",
       "      <td>0.151560</td>\n",
       "      <td>0.136270</td>\n",
       "      <td>-0.48813</td>\n",
       "      <td>0.482230</td>\n",
       "      <td>-0.095715</td>\n",
       "      <td>0.183060</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.46434</td>\n",
       "      <td>0.32394</td>\n",
       "      <td>0.25984</td>\n",
       "      <td>0.40849</td>\n",
       "      <td>0.203510</td>\n",
       "      <td>0.058722</td>\n",
       "      <td>-0.164080</td>\n",
       "      <td>0.20672</td>\n",
       "      <td>-0.184400</td>\n",
       "      <td>0.071147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>0.94911</td>\n",
       "      <td>-0.34968</td>\n",
       "      <td>0.48125</td>\n",
       "      <td>-0.19306</td>\n",
       "      <td>-0.008838</td>\n",
       "      <td>0.281820</td>\n",
       "      <td>-0.96130</td>\n",
       "      <td>-0.135810</td>\n",
       "      <td>-0.430830</td>\n",
       "      <td>-0.092933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.80127</td>\n",
       "      <td>0.30831</td>\n",
       "      <td>0.43567</td>\n",
       "      <td>0.88747</td>\n",
       "      <td>0.298160</td>\n",
       "      <td>-0.024650</td>\n",
       "      <td>-0.950750</td>\n",
       "      <td>0.36233</td>\n",
       "      <td>-0.725120</td>\n",
       "      <td>-0.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>done</th>\n",
       "      <td>0.33076</td>\n",
       "      <td>-0.43870</td>\n",
       "      <td>-0.32163</td>\n",
       "      <td>-0.49310</td>\n",
       "      <td>0.102540</td>\n",
       "      <td>-0.002742</td>\n",
       "      <td>-0.51720</td>\n",
       "      <td>0.024336</td>\n",
       "      <td>-0.128160</td>\n",
       "      <td>0.143490</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26668</td>\n",
       "      <td>-0.17660</td>\n",
       "      <td>0.01582</td>\n",
       "      <td>0.25528</td>\n",
       "      <td>-0.096739</td>\n",
       "      <td>-0.097282</td>\n",
       "      <td>-0.084483</td>\n",
       "      <td>0.33312</td>\n",
       "      <td>-0.222520</td>\n",
       "      <td>0.744570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>better.</th>\n",
       "      <td>-0.12090</td>\n",
       "      <td>-0.16821</td>\n",
       "      <td>0.24099</td>\n",
       "      <td>-0.30287</td>\n",
       "      <td>0.435780</td>\n",
       "      <td>-0.383670</td>\n",
       "      <td>-0.55203</td>\n",
       "      <td>-0.286810</td>\n",
       "      <td>-0.100920</td>\n",
       "      <td>0.477690</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.36585</td>\n",
       "      <td>-0.10114</td>\n",
       "      <td>0.40423</td>\n",
       "      <td>0.25951</td>\n",
       "      <td>0.087927</td>\n",
       "      <td>0.061960</td>\n",
       "      <td>0.075266</td>\n",
       "      <td>0.12755</td>\n",
       "      <td>0.066461</td>\n",
       "      <td>1.116300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1        2        3         4         5        6   \\\n",
       "could    0.90754 -0.38322  0.67648 -0.20222  0.151560  0.136270 -0.48813   \n",
       "have     0.94911 -0.34968  0.48125 -0.19306 -0.008838  0.281820 -0.96130   \n",
       "done     0.33076 -0.43870 -0.32163 -0.49310  0.102540 -0.002742 -0.51720   \n",
       "better. -0.12090 -0.16821  0.24099 -0.30287  0.435780 -0.383670 -0.55203   \n",
       "\n",
       "               7         8         9   ...       40       41       42  \\\n",
       "could    0.482230 -0.095715  0.183060  ... -0.46434  0.32394  0.25984   \n",
       "have    -0.135810 -0.430830 -0.092933  ... -0.80127  0.30831  0.43567   \n",
       "done     0.024336 -0.128160  0.143490  ... -0.26668 -0.17660  0.01582   \n",
       "better. -0.286810 -0.100920  0.477690  ... -0.36585 -0.10114  0.40423   \n",
       "\n",
       "              43        44        45        46       47        48        49  \n",
       "could    0.40849  0.203510  0.058722 -0.164080  0.20672 -0.184400  0.071147  \n",
       "have     0.88747  0.298160 -0.024650 -0.950750  0.36233 -0.725120 -0.608900  \n",
       "done     0.25528 -0.096739 -0.097282 -0.084483  0.33312 -0.222520  0.744570  \n",
       "better.  0.25951  0.087927  0.061960  0.075266  0.12755  0.066461  1.116300  \n",
       "\n",
       "[4 rows x 50 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# structure the embeddings of each word in a sentence\n",
    "sentence_embedding = pd.DataFrame(\n",
    "    [embedding_matrix[word_id] for word_id in sentence_ids][0],  # store the embedding of each word in a list\n",
    "    index=EXAMPLE_SENTENCE.split()  # use the words (from the original sentence) as the row index\n",
    ")\n",
    "\n",
    "sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### average the embeddings as a simple example of how to combine embeddings\n",
    "\n",
    "[A Simple but Tough-To-Beat Baseline for Sentence Embeddings](https://openreview.net/pdf?id=SyK00v5xx):\n",
    "Sanjeev Arora, Yingyu Liang, Tengyu Ma \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.516627</td>\n",
       "      <td>-0.334952</td>\n",
       "      <td>0.269272</td>\n",
       "      <td>-0.297812</td>\n",
       "      <td>0.17026</td>\n",
       "      <td>0.007919</td>\n",
       "      <td>-0.629665</td>\n",
       "      <td>0.020986</td>\n",
       "      <td>-0.188906</td>\n",
       "      <td>0.177827</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.474535</td>\n",
       "      <td>0.088628</td>\n",
       "      <td>0.27889</td>\n",
       "      <td>0.452688</td>\n",
       "      <td>0.123214</td>\n",
       "      <td>-0.000312</td>\n",
       "      <td>-0.281012</td>\n",
       "      <td>0.25743</td>\n",
       "      <td>-0.266395</td>\n",
       "      <td>0.330779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3        4         5         6   \\\n",
       "0  0.516627 -0.334952  0.269272 -0.297812  0.17026  0.007919 -0.629665   \n",
       "\n",
       "         7         8         9   ...        40        41       42        43  \\\n",
       "0  0.020986 -0.188906  0.177827  ... -0.474535  0.088628  0.27889  0.452688   \n",
       "\n",
       "         44        45        46       47        48        49  \n",
       "0  0.123214 -0.000312 -0.281012  0.25743 -0.266395  0.330779  \n",
       "\n",
       "[1 rows x 50 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sentence_embedding.mean()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reminder - number of words learned from the vocabulary\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding layer from the pre-trained embeddings\n",
    "\n",
    "e = Embedding(\n",
    "  input_dim=vocab_size,            # input (valid vocabulary) size\n",
    "  output_dim=50,                   # output size (dimensionality of pre-trained embeddings)\n",
    "  weights=[embedding_matrix],      # add pre-trained embeddings\n",
    "  input_length=4,                  # Length of input sequences, when it is constant.\n",
    "  trainable=False                  # prevent updates to the prtrained embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add embedding to the model\n",
    "model.add(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras offers an Embedding layer that can be used for neural networks on text data. It requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
    "\n",
    "**The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.**\n",
    "\n",
    "It is a flexible layer that can be used in a variety of ways, such as:\n",
    "\n",
    "    - It can be used alone to learn a word embedding that can be saved and used in another model later.\n",
    "    - It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
    "    - It can be used to load a pre-trained word embedding model, a type of transfer learning.\n",
    "\n",
    "\n",
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "\n",
    "- **input_dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "\n",
    "\n",
    "- **output_dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "\n",
    "\n",
    "- **input_length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "For example, below we define an Embedding layer with a vocabulary of 200 (e.g. integer encoded words from 0 to 199, inclusive), a vector space of 32 dimensions in which words will be embedded, and input documents that have 50 words each.\n",
    "\n",
    "The Embedding layer has weights that are learned. If you save your model to file, this will include weights for the Embedding layer. The output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document).\n",
    "\n",
    "SOURCES: \n",
    "- https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "- https://keras.io/layers/embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten\n",
    "\n",
    "Use Flatten to convert a multidimensional tensor into a single 1-D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Flatten example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n"
     ]
    }
   ],
   "source": [
    "print(np.arange(0,24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7]],\n",
       "\n",
       "        [[ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.arange(0,24).reshape(1,3,2,4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      "  18. 19. 20. 21. 22. 23.]]\n"
     ]
    }
   ],
   "source": [
    "# define a model that only flattens the data\n",
    "inputs = Input(shape=(3,2,4))\n",
    "output = Flatten()(inputs)\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# view the flattened output\n",
    "print(model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-270-2d74ce9baa04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Model' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 4, 50)             800       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 1,001\n",
      "Trainable params: 201\n",
      "Non-trainable params: 800\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 12 input samples and 10 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-182-ecb4aa393b45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    802\u001b[0m             ]\n\u001b[0;32m    803\u001b[0m             \u001b[1;31m# Check that all arrays have the same length.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 804\u001b[1;33m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    805\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m                 \u001b[1;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    235\u001b[0m                          \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                          \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    238\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 12 input samples and 10 target samples."
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-48-89a65a8573c8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-48-89a65a8573c8>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    https://github.com/zalandoresearch/flair/blob/master/flair/embeddings.py\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://github.com/zalandoresearch/flair/blob/master/flair/embeddings.py\n",
    "    \n",
    "embedding example: \n",
    "#    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp3",
   "language": "python",
   "name": "nlp3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
