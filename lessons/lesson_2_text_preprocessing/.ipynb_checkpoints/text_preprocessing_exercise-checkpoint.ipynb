{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Exercise\n",
    "##### Author: Alex Sherman | alsherman@deloitte.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agenda\n",
    "\n",
    "1. SpaCy\n",
    "2. Text Tokenization, POS Tagging, Parsing, NER\n",
    "3. Text Pipelines\n",
    "4. Phrase Models\n",
    "5. Python Fundamentals: Collections, Itertools, list comprehensions, sorted, apply\n",
    "6. Text Rule-based matching\n",
    "7. Advanced SpaCy Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import Image\n",
    "from configparser import ConfigParser, ExtendedInterpolation\n",
    "\n",
    "config = ConfigParser(interpolation=ExtendedInterpolation())\n",
    "config.read('../../config.ini')\n",
    "DB_PATH = config['DATABASES']['PROJECT_DB_PATH']\n",
    "CLEANED_TEXT_PATH = config['NLP']['CLEANED_TEXT_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm DB_PATH is in the correct db directory, otherwise the rest of the code will not work\n",
    "DB_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the names of the tables in the database\n",
    "engine = create_engine(DB_PATH)\n",
    "pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql(\"SELECT COUNT(*) FROM pubmed \", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the oracle 10k documents \n",
    "df = pd.read_sql(\n",
    "      \"SELECT * FROM pubmed\"\n",
    "    , index_col='index'\n",
    "    , con=engine\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the number of characters displayed in each column\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text\n",
    "text = df.text[27]\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "\n",
    "\"SpaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "\n",
    "If you're working with a lot of text, you'll eventually want to know more about it. For example, what's it about? What do the words mean in context? Who is doing what to whom? What companies and products are mentioned? Which texts are similar to each other?\n",
    "\n",
    "SpaCy is designed specifically for production use and helps you build applications that process and \"understand\" large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
    "\n",
    "SpaCy is not research software. It's built on the latest research, but it's designed to get things done. This leads to fairly different design decisions than NLTK or CoreNLP, which were created as platforms for teaching and research. The main difference is that SpaCy is integrated and opinionated. SpaCy tries to avoid asking the user to choose between multiple algorithms that deliver equivalent functionality. Keeping the menu small lets SpaCy deliver generally better performance and developer experience.\"\n",
    "\n",
    "### SpaCy Features \n",
    "\n",
    "NAME |\tDESCRIPTION |\n",
    ":----- |:------|\n",
    "Tokenization|Segmenting text into words, punctuations marks etc.|\n",
    "Part-of-speech (POS) Tagging|Assigning word types to tokens, like verb or noun.|\n",
    "Dependency Parsing|\tAssigning syntactic dependency labels, describing the relations between individual tokens, like subject or object.|\n",
    "Lemmatization|\tAssigning the base forms of words. For example, the lemma of \"was\" is \"be\", and the lemma of \"rats\" is \"rat\".|\n",
    "Sentence Boundary Detection (SBD)|\tFinding and segmenting individual sentences.|\n",
    "Named Entity Recognition (NER)|\tLabelling named \"real-world\" objects, like persons, companies or locations.|\n",
    "Similarity|\tComparing words, text spans and documents and how similar they are to each other.|\n",
    "Text Classification|\tAssigning categories or labels to a whole document, or parts of a document.|\n",
    "Rule-based Matching|\tFinding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions.|\n",
    "Training|\tUpdating and improving a statistical model's predictions.|\n",
    "Serialization|\tSaving objects to files or byte strings.|\n",
    "\n",
    "SOURCE: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy Installation:\n",
    "- Windows: Download Microsoft Visual C++: \n",
    "1.\tGo to: https://www.visualstudio.com/downloads/#build-tools-for-visual-studio-2017\n",
    "2.\tDownload the first link for Visual Studio Community 2017\n",
    "3.\tDuring the install select the option to install Desktop with Development C++ (see image below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desktop with Development C++\n",
    "Image(\"../../raw_data/images/visual_studio_community.png\", width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SpaCy Installation\n",
    "Run the following using git bash as an administrator (i.e. right click on the git bash logo and select 'Run as Admin')\n",
    "- conda install -c conda-forge spacy\n",
    "- python -m spacy download en\n",
    "\n",
    "##### if you run into an error try the following:\n",
    "- python -m spacy link en_core_web_sm en\n",
    "- SOURCE: https://github.com/explosion/spaCy/issues/950\n",
    "\n",
    "##### Optional to install a convolutional neural network model  (~800MB). This is the model I will use in class:\n",
    "- python -m spacy download en_core_web_lg\n",
    "\n",
    "##### Test the following code from git bash (even if previous step failed):\n",
    "start python\n",
    "- python -i\n",
    "\n",
    "test if SpaCy was downloaded\n",
    "- import spacy\n",
    "\n",
    "approach 1: test if model downloaded\n",
    "- nlp = spacy.load('en') \n",
    "\n",
    "appraoch 2: test this if spacy.load('en') failed\n",
    "- import en_core_web_sm\n",
    "- nlp = en_core_web_sm.load()\n",
    "\n",
    "exit Python\n",
    "- exit()\n",
    "\n",
    "\n",
    "##### Optional - install on an AWS EC2 instance\n",
    "Instance: Amazon Linux 2 LTS Candidate 2 AMI (HVM), SSD Volume Type\n",
    "\n",
    "- #!/bin/bash\n",
    "- sudo yum update -y\n",
    "- sudo yum groupinstall 'Development Tools' -y\n",
    "- sudo easy_install pip\n",
    "- sudo yum install python-devel -y\n",
    "- sudo pip install spacy\n",
    "- sudo python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm which conda environment you are using - make sure it is one with SpaCy installed\n",
    "import sys\n",
    "sys.executable\n",
    "\n",
    "# if you have difficulty importing spacy try the following in git bash\n",
    "# conda install ipykernel --name Python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# read in a simple (small) English language model\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# another approach:\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# read in a (large) convolutional neural network model\n",
    "# this will only work after the CNN model is downloaded (~800MB)\n",
    "# e.g. python -m spacy download en_core_web_lg\n",
    "nlp = spacy.load('en_core_web_lg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the document text\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the text from the SpaCy object\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which the SpaCy document methods and attributes\n",
    "print(dir(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Pipeline\n",
    "\n",
    "When you read the text into spaCy, e.g. doc = nlp(text), you are applying a pipeline of nlp processes to the text.\n",
    "by default spaCy applies a tagger, parser, and ner, but you can choose to add, replace, or remove these steps.\n",
    "Note: Removing unnecessary steps for a given nlp can lead to substantial descreses in processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy pipeline\n",
    "spacy_url = 'https://spacy.io/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg'\n",
    "iframe = '<iframe src={} width=1000 height=200></iframe>'.format(spacy_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "SpaCy first tokenizes the text, i.e. segments it into words, punctuation and so on. This is done by applying rules specific to each language. For example, punctuation at the end of a sentence should be split off – whereas \"U.K.\" should remain one token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenization_url = 'https://spacy.io/tokenization-57e618bd79d933c4ccd308b5739062d6.svg'\n",
    "iframe = '<iframe src={} width=650 height=400></iframe>'.format(tokenization_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lexeme - entries in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a list of stop words from SpaCy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "print('Example stop words: {}'.format(list(STOP_WORDS)[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['have']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(nlp.vocab['have']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab['have'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for word in the SpaCy vocabulary and\n",
    "# change the is_stop attribute to True\n",
    "\n",
    "for word in STOP_WORDS:\n",
    "    nlp.vocab[word].is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech (POS) Tagging\n",
    "\n",
    "After tokenization, spaCy can parse and tag a given Doc. This is where the statistical model comes in, which enables spaCy to make a prediction of which tag or label most likely applies in this context. A model consists of binary data and is produced by showing a system enough examples for it to make predictions that generalize across the language – for example, a word following \"the\" in English is most likely a noun.\n",
    "\n",
    "Annotation | Description\n",
    ":----- |:------|\n",
    "Text |The original word text|\n",
    "Lemma |The base form of the word.|\n",
    "POS |The simple part-of-speech tag.|\n",
    "Tag |The detailed part-of-speech tag.|\n",
    "Dep |Syntactic dependency, i.e. the relation between tokens.|\n",
    "Shape |The word shape – capitalisation, punctuation, digits.|\n",
    "Is Alpha |Is the token an alpha character?|\n",
    "Is Stop |Is the token part of a stop list, i.e. the most common words of the language?|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review document\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if POS tags were added to the doc in the NLP pipeline\n",
    "doc.is_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print column headers\n",
    "print('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} | '.format(\n",
    "    'TEXT','LEMMA_','POS_','TAG_','DEP_','SHAPE_','IS_ALPHA','IS_STOP'))\n",
    "\n",
    "# print various SpaCy POS attributes\n",
    "for token in doc:\n",
    "    print('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} |'.format(\n",
    "          token.text, token.lemma_, token.pos_, token.tag_, token.dep_\n",
    "        , token.shape_, token.is_alpha, token.is_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create (adjective --> noun) phrases from parts of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_token = doc[0]  # set first token\n",
    "\n",
    "for token in doc[1:]:    \n",
    "    # identify adjective noun pairs\n",
    "    if previous_token.pos_ == 'ADJ' and token.pos_ == 'NOUN':\n",
    "        print(f'{previous_token.text}_{token.text}')\n",
    "    \n",
    "    previous_token = token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### word sense disambiguation via part of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[0:20]:\n",
    "    print(f'{token.text}_{token.pos_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Dependency Parsing\n",
    "\n",
    "spaCy features a fast and accurate syntactic dependency parser, and has a rich API for navigating the tree. The parser also powers the sentence boundary detection, and lets you iterate over base noun phrases, or \"chunks\". You can check whether a Doc  object has been parsed with the doc.is_parsed attribute, which returns a boolean value. If this attribute is False, the default sentence iterator will raise an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check is document has been parsed (dependency parsing)\n",
    "doc.is_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:15} | {:10} | {:15} | {:10} | {:25} | {:25}'.format(\n",
    "    'TEXT','DEP','HEAD TEXT','HEAD POS','CHILDREN','LEFTS'))\n",
    "\n",
    "for token in doc:\n",
    "    print('{:15} | {:10} | {:15} | {:10} | {:25} | {:25}'.format(\n",
    "        token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "        str([child for child in token.children]), str([t.text for t in token.lefts])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOUN CHUNCKS:\n",
    "\n",
    "| **TERM** | Definition |\n",
    "|:---|:---:|\n",
    "| **Text** | The original noun chunk text |\n",
    "| **Root text** | The original text of the word connecting the noun chunk to the rest of the parse |\n",
    "| **Root dependency** | Dependency relation connecting the root to its head |\n",
    "| **Root head text** | The text of the root token's head |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:15} | {:10} | {:15} | {:40}'.format('ROOT_TEXT','ROOT','DEPENDENCY','TEXT'))\n",
    "\n",
    "for chunk in list(doc.noun_chunks):\n",
    "    print('{:15} | {:10} | {:15} | {:40}'.format(\n",
    "        chunk.root.text, chunk.root.dep_, chunk.root.head.text, chunk.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency visualization\n",
    "\n",
    "# show visualization in Jupyter Notebook\n",
    "displacy.render(docs=doc, style='dep', jupyter=True)\n",
    "\n",
    "# Another Option\n",
    "# uncomment and run the below code, then open another browser tab and go to http://localhost:5000\n",
    "# when you are done (before you run the next cell in the notebook) stop this cell\n",
    "# displacy.serve(docs=doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)\n",
    "\n",
    "A named entity is a \"real-world object\" that's assigned a name – for example, a person, a country, a product, or a book title. spaCy can recognise various types of named entities in a document, by asking the model for a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_text = \"When I told John that I wanted to move to Alaska, he warned me that I'd have trouble finding a Starbucks there.\"\n",
    "ner_doc = nlp(ner_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:10} | {:15}'.format('LABEL','ENTITY'))\n",
    "\n",
    "for ent in ner_doc.ents[0:20]:\n",
    "    print('{:10} | {:50}'.format(ent.label_, ent.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ent methods and attributes\n",
    "print(dir(ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity visualization\n",
    "# after you run this code, open another browser and go to http://localhost:5000\n",
    "# when you are done (before you run the next cell in the notebook) stop this cell \n",
    "\n",
    "displacy.render(docs=ner_doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "If you have a sequence of documents to process, you should use the Language.pipe()  method. The method takes an iterator of texts, and accumulates an internal buffer, which it works on in parallel. It then yields the documents in order, one-by-one.\n",
    "\n",
    "- batch_size: number of docs to process per thread\n",
    "- disable: Names of pipeline components to disable to speed up text processing.\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with a subset of the data, mentioning the word immune\n",
    "immune_df = df[df.text.str.contains('immune')].text\n",
    "\n",
    "# print the count of matches\n",
    "print('Lines with the term immune: {}\\n'.format(len(immune_df)))\n",
    "\n",
    "# view the first five section names\n",
    "for line in immune_df.head(2):\n",
    "    print(line, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for doc in nlp.pipe(immune_df.head(10)):  # includes ['parser','tagger','ner']\n",
    "    if 'immune' in doc.text:\n",
    "        print(doc, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy - Tips for faster processing\n",
    "\n",
    "You can substantially speed up the time it takes SpaCy to read a document by disabling components of the NLP that are not necessary for a given task.\n",
    "\n",
    "- Disable options: **parser, tagger, ner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# processing occurs ~75x faster by disabling pipeline components\n",
    "for doc in nlp.pipe(immune_df.head(10), disable=['parser','tagger','ner']):\n",
    "    if 'immune' in doc.text:\n",
    "        print(doc, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Determine which NLP components can be disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_pos(doc, n_tokens=5):\n",
    "    \"\"\" print SpaCy POS information about each token in a provided document \"\"\"\n",
    "    print('{:15} | {:10} | {:10} | {:30}'.format('TOKEN','POS','DEP_','LEFTS'))\n",
    "    for token in doc[0:n_tokens]:\n",
    "        print('{:15} | {:10} | {:10} | {:30}'.format(\n",
    "            token.text, token.head.pos_,token.dep_, str([t.text for t in token.lefts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe results from the default pipeline\n",
    "pos_doc = nlp(text)\n",
    "view_pos(pos_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by parser\n",
    "pos_doc = nlp(text, disable=['ner','parser'])\n",
    "view_pos(pos_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe which part of speech (pos) attributes are disabled by tagger\n",
    "pos_doc = nlp(text, disable=['ner','tagger'])\n",
    "view_pos(pos_doc, n_tokens=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Sentence Boundary Detection (SBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp_sbd = English()  # just the language with no model\n",
    "\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "nlp_sbd.add_pipe(sentencizer)\n",
    "doc = nlp_sbd(\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. print all the distinct entities tagged with 'CARDINAL'\n",
    "2. print all the distinct entities tagged with 'PERSON'\n",
    "3. print all the distinct entities tagged with 'GPE'\n",
    "\n",
    "For all exercises:\n",
    "- use a batch size of 100\n",
    "- disable the parser and tagger (ner is needed to add the tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the distinct entities tagged as a CARDINAL\n",
    "# search in immune_df.head(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the distinct entities tagged as an organization (ORG)\n",
    "# search in immune_df.head(500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the distinct entities tagged as a geopolitical entity (GPE)\n",
    "# search in immune_df.head(1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "\n",
    "\"A collocation is an expression consisting of two or more words that\n",
    "correspond to some conventional way of saying things. Or in the words\n",
    "of Firth (1957: 181): “Collocations of a given word are statements of the\n",
    "habitual or customary places of that word.” Collocations include noun\n",
    "phrases like strong tea and weapons of mass destruction, phrasal verbs like\n",
    "to make up, and other stock phrases like the rich and powerful. Particularly\n",
    "interesting are the subtle and not-easily-explainable patterns of word usage\n",
    "that native speakers all know: why we say a stiff breeze but not a stiff wind\n",
    "(while either a strong breeze or a strong wind is okay), or why we speak of\n",
    "broad daylight (but not bright daylight or narrow darkness)\n",
    "\n",
    "\n",
    "\n",
    "There are actually different definitions of the notion of collocation. Some\n",
    "authors in the computational and statistical literature define a collocation\n",
    "as two or more consecutive words with a special behavior, for example\n",
    "Choueka (1988):\n",
    "[A collocation is defined as] a sequence of two or more consecutive\n",
    "words, that has characteristics of a syntactic and semantic\n",
    "unit, and whose exact and unambiguous meaning or connotation\n",
    "cannot be derived directly from the meaning or connotation of its\n",
    "components. In most linguistically oriented research, a phrase\n",
    "can be a collocation even if it is not consecutive (as in the example knock\n",
    ". . . door). The following criteria are typical of linguistic treatments of collocations:\n",
    "\n",
    "**Non-compositionality**: The meaning of a collocation is not a straightforward\n",
    "composition of the meanings of its parts. Either the meaning\n",
    "is completely different from the free combination (as in the case of idioms\n",
    "like kick the bucket) or there is a connotation or added element of\n",
    "meaning that cannot be predicted from the parts. For example, white\n",
    "wine, white hair and white woman all refer to slightly different colors, so\n",
    "we can regard them as collocations. \n",
    "\n",
    "**Non-substitutability**: We cannot substitute near-synonyms for the\n",
    "components of a colloction. For example, we can’t say yellow wine\n",
    "instead of white wine even though yellow is as good a description of the\n",
    "color of white wine as white is (it is kind of a yellowish white).\n",
    "\n",
    "**Non-modifiability**: Many collocations cannot be freely modified with\n",
    "additional lexical material or through grammatical transformations.\n",
    "This is especially true for frozen expressions like idioms. For example,\n",
    "we can’t modify frog in to get a frog in one’s throat into to get an ugly\n",
    "frog in one’s throat although usually nouns like frog can be modified by\n",
    "adjectives like ugly. Similarly, going from singular to plural can make\n",
    "an idiom ill-formed, for example in people as poor as church mice.\"\n",
    "\n",
    "SOURCE: https://nlp.stanford.edu/fsnlp/promo/colloc.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase (collocation) Detection\n",
    "\n",
    "Phrase modeling is another approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our reviews and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. The formula our phrase models will use to determine whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "$$\\frac{count(A\\ B) - count_{min}}{count(A) * count(B)} > threshold$$\n",
    "\n",
    "- $count(A\\ B)$ is the number of times the tokens $A\\ B$ appear in the corpus in order\n",
    "- $count_{min}$ is a user-defined parameter to ensure that accepted phrases occur a minimum number of times\n",
    "- $count(A)$ is the number of times token $A$ appears in the corpus\n",
    "- $count(B)$ is the number of times token $B$ appears in the corpus\n",
    "- $threshold$ is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase\n",
    "\n",
    "Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "\n",
    "Phrase modeling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model (so new york would become new_york). But you would also expect multi-word expressions that represent common concepts, but aren't specifically named entities (such as happy hour) to also become phrases in the model.\n",
    "\n",
    "We turn to the indispensible gensim library to help us with phrase modeling — the Phrases class in particular.\n",
    "\n",
    "SOURCE: \n",
    "- https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb\n",
    "- https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gensim API\n",
    "A more complex API, though it is faster and has better integration with other gensim components (e.g. Phraser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gensim simple_preprocess to preprocess text\n",
    "for text in immune_df:\n",
    "    print(text, '\\n')\n",
    "    print(simple_preprocess(text))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gensim_text = [simple_preprocess(text) for text in immune_df]\n",
    "\n",
    "print(gensim_text[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "common_terms = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**common_terms:** optional list of “stop words” that won’t affect frequency count of expressions containing them.\n",
    "- The common_terms parameter add a way to give special treatment to common terms (aka stop words) such that their presence between two words won’t prevent bigram detection. It allows to detect expressions like “bank of america” or “eye of the beholder”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(\n",
    "      gensim_text\n",
    "    , common_terms=common_terms\n",
    "    , min_count=5\n",
    "    , threshold=5\n",
    "    , scoring='default'\n",
    ")\n",
    "\n",
    "phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrases Params\n",
    "\n",
    "- **scoring:** specifies how potential phrases are scored for comparison to the threshold setting. scoring can be set with either a string that refers to a built-in scoring function, or with a function with the expected parameter names. Two built-in scoring functions are available by setting scoring to a string:\n",
    "\n",
    "    - ‘default’: from “Efficient Estimaton of Word Representations in Vector Space” by Mikolov, et. al.: \n",
    "    \n",
    "$$\\frac{count(AB) - count_{min}}{count(A) * count(B)} * N > threshold$$\n",
    "    \n",
    "\n",
    "\n",
    "    - where N is the total vocabulary size.\n",
    "    - Thus, it is easier to exceed the threshold when the two words occur together often or when the two words are rare (i.e. small product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "\n",
    "bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The phrases object still contains all the source text in memory. A gensim Phraser will remove this extra data to become smaller and somewhat faster than using the full Phrases model. To determine what data to remove, the Phraser ues the  results of the source model’s min_count, threshold, and scoring settings. (You can tamper with those & create a new Phraser to try other values.)\n",
    "\n",
    "SOURCE: https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_phrases(phraser, text_stream, num_underscores=2):\n",
    "    \"\"\" identify phrases from a text stream by searching for terms that\n",
    "        are separated by underscores and include at least num_underscores\n",
    "    \"\"\"\n",
    "    \n",
    "    phrases = []\n",
    "    for terms in phraser[text_stream]:\n",
    "        for term in terms:\n",
    "            if term.count('_') >= num_underscores:\n",
    "                phrases.append(term)\n",
    "    print(set(phrases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_phrases(bigram, gensim_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tri-gram phrase model\n",
    "\n",
    "We can place the text from the first phrase model into another Phrases object to create n-term phrase models. We can repear this process multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(\n",
    "      bigram[gensim_text]\n",
    "    , common_terms=common_terms\n",
    "    , min_count=1\n",
    "    , threshold=1\n",
    ")\n",
    "\n",
    "trigram = Phraser(phrases)\n",
    "\n",
    "print_phrases(trigram, bigram[gensim_text], num_underscores=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_num in [0]:\n",
    "    print('DOC NUMBER: {}\\n'.format(doc_num))\n",
    "    print('ORIGINAL SENTENT: {}\\n'.format(' '.join(gensim_text[doc_num])))\n",
    "    print('BIGRAM: {}\\n'.format(' '.join(bigram[gensim_text[doc_num]])))\n",
    "    print('TRIGRAM: {}'.format(' '.join(trigram[bigram[gensim_text[doc_num]]])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the cleaned text to a new file for later use\n",
    "\n",
    "#with open(CLEANED_TEXT_PATH, 'w') as f:\n",
    "#    for line in bigram[gensim_text]:\n",
    "#        line = ' '.join(line) + '\\n'\n",
    "#        line = line.encode('ascii', errors='ignore').decode('ascii')\n",
    "#        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Python\n",
    "\n",
    "A brief overview of some advanced Python which will be used in future lessons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collections - DefaultDict\n",
    "\n",
    "Usually, a Python dictionary throws a KeyError if you try to get an item with a key that is not currently in the dictionary. The defaultdict in contrast will simply create any items that you try to access (provided of course they do not exist yet). To create such a \"default\" item, it calls the function object that you pass in the constructor (more precisely, it's an arbitrary \"callable\" object, which includes function and type objects). For the first example, default items are created using int(), which will return the integer object 0. For the second example, default items are created using list(), which returns a new empty list object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_sentence = \"\"\"PubMed Description: \n",
    "PubMed comprises more than 28 million citations for biomedical literature from MEDLINE, life science journals, and online books.\n",
    "Citations may include links to full-text content from PubMed Central and publisher web sites.\"\"\".strip()\n",
    "\n",
    "example_doc = nlp(pubmed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG APPROACH - KeyError!\n",
    "\n",
    "# try to create a word count dict with new keys\n",
    "d = {}\n",
    "for word in example_doc:\n",
    "    d[word] += 1  # cannot add if the key does not exist\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = defaultdict(int)  # define the type of data the dict stores\n",
    "for word in example_doc:\n",
    "    d[word.text] += 1  # can add to unassigned keys\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collections - Counter\n",
    "\n",
    "A Counter is a dict subclass for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other languages.\n",
    "\n",
    "SOURCE: https://docs.python.org/2/library/collections.html#collections.Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of times each CARDINAL appears\n",
    "print(Counter(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Iterrtools - combinations\n",
    "\n",
    "\"The itertools module standardizes a core set of fast, memory efficient tools that are useful by themselves or in combination. Together, they form an “iterator algebra” making it possible to construct specialized tools succinctly and efficiently in pure Python.\n",
    "\n",
    "**Combinations**\n",
    "- Return r length subsequences of elements from the input iterable.\n",
    "- Combinations are emitted in lexicographic sort order. So, if the input iterable is sorted, the combination tuples will be produced in sorted order.\n",
    "- Elements are treated as unique based on their position, not on their value. So if the input elements are unique, there will be no repeat values in each combination.\n",
    "\n",
    "SOURCE: https://docs.python.org/3.4/library/itertools.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['PubMed','Medline','Citation','Biomedical']\n",
    "for combo in combinations(terms, 2):\n",
    "    print(combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List Comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traditional iteration\n",
    "\n",
    "terms_subset = []\n",
    "for term in terms:\n",
    "    if 'med' in term.lower():\n",
    "        terms_subset.append(term)\n",
    "\n",
    "terms_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list comprehension\n",
    "\n",
    "#   return value   iteration           conditional\n",
    "#[  term           for term in terms   if 'med' in term.lower()]\n",
    "\n",
    "[term for term in terms if 'med' in term.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sorted\n",
    "\n",
    "sorted(iterable, key=None, reverse=False)\n",
    "\n",
    "- Return a new sorted list from the items in iterable.\n",
    "- Has two optional arguments which must be specified as keyword arguments.\n",
    "- key specifies a function of one argument that is used to extract a comparison key from each list element: key=str.lower. The default value is None (compare the elements directly).\n",
    "- reverse is a boolean value. If set to True, then the list elements are sorted as if each comparison were reversed.\n",
    "\n",
    "SOURCE: https://docs.python.org/3/library/functions.html#sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles =[('article2', 3),('article3', 2),('article1', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(articles, key=lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(articles, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort based on the last character of the first term\n",
    "sorted(articles, key=lambda x:x[0][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pandas Apply\n",
    "\n",
    "apply is an efficient and fast approach to 'apply' a function to every element in a row. applymap does the same to every element in the entire dataframe (e.g. convert all ints to floats)\n",
    "\n",
    "Example: https://chrisalbon.com/python/data_wrangling/pandas_apply_operations_to_dataframes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a small dataframe with example data\n",
    "example_data = {'col1':range(0,3),'col2':range(3,6)}\n",
    "test_df = pd.DataFrame(example_data)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a built-in function to each element in a column\n",
    "test_df['col1'].apply(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a custom function to every element in a column\n",
    "def add_five(row):\n",
    "    return row + 5\n",
    "\n",
    "test_df['col1'].apply(add_five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply an annonomous function to every element in a column\n",
    "test_df['col1'].apply(lambda x: x+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a built-in function to every element in a dataframe \n",
    "test_df.applymap(float)  # applymap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to on which to apply a function\n",
    "disease_df = df[df.text.str.contains('disease')].copy()\n",
    "disease_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to apply to the dataframe\n",
    "def noun_count(text):\n",
    "    \"\"\" count the number of nouns in the provided text\n",
    "    \n",
    "    :param text: input text\n",
    "    :return num_nouns: number of nouns in the text\n",
    "    \"\"\"\n",
    "\n",
    "    num_nouns = 0\n",
    "    doc = nlp(text, disable=['ner'])\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            num_nouns += 1\n",
    "            \n",
    "    return num_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# apply the function to the dataframe to create a new columns\n",
    "immune_df['noun_count'] = immune_df.text.apply(noun_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immune_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. Count how many time each individual entity appears\n",
    "2. Create a mapping that keeps track of every combination of entities pairs that appear in the same sentence\n",
    "3. Count how many times each entity combo appears\n",
    "4. Print the entity combos (using sorted) in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create a defaultdict(int) called entity_relations\n",
    "entity_relations = \n",
    "\n",
    "# create an empty list called counter_entities \n",
    "counter_entities = \n",
    "\n",
    "# during testing set .head() to a smaller number such as .head(1000) \n",
    "for doc in nlp.pipe(immune_df.head(1000), disable=['parser','tagger', 'ner']):\n",
    "\n",
    "    # store the token.text for all the tokens containing the letters 'toxic' (i.e. 'toxic' in term)\n",
    "    # use a list comprehension\n",
    "    entities = \n",
    "\n",
    "    # add the tokens from the current doc to counter_entities (use += to add the token.text)\n",
    "    counter_entities \n",
    "    \n",
    "    # create combinations of two terms each time multiple 'toxic' words appear\n",
    "    # increment the count in entity_relations defaultdict each time a combo is repeated\n",
    "    for combo in combinations(entities, 2):\n",
    "        entity_relations[combo] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(counter_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the entity pairs in descending order\n",
    "sorted(entity_relations.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Identify Relevant Text (Rule-based Matching)\n",
    "\n",
    "Finding sequences of tokens based on their texts and linguistic annotations, similar to regular expressions. We will use this to filter and extract relevant text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=https://spacy.io/usage/linguistic-features#rule-based-matching width=1000 height=700></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rule_basesd_matching_url = 'https://spacy.io/usage/linguistic-features#rule-based-matching'\n",
    "iframe = '<iframe src={} width=1000 height=700></iframe>'.format(rule_basesd_matching_url)\n",
    "HTML(iframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Matcher identifies text from rules we specify\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to specify what to do with the matching text\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    \"\"\"  collect and transform matching text\n",
    "\n",
    "    :param matcher: Matcher object\n",
    "    :param doc: is the full document to search for text patterns\n",
    "    :param i: is the index of the text matches\n",
    "    :param matches: matches found in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    match_id, start, end = matches[i]  # indices of matched term\n",
    "    span = doc[start:end]              # extract matched term\n",
    "    \n",
    "    print('span: {} | start_ind:{:5} | end_ind:{:5} | id:{}'.format(\n",
    "        span, start, end, match_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# set a pattern of text to collect\n",
    "# find all mentions of the word\n",
    "pattern = [{'LOWER':'disease'}] # LOWER coverts words to lowercase before matching\n",
    "\n",
    "# instantiate matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# add pattern to the matcher (one matcher can look for many unique patterns)\n",
    "# provice a pattern name, function to apply to matches, pattern to identify\n",
    "matcher.add('disease', collect_sents, pattern)\n",
    "\n",
    "# pass the doc to the matcher to run the collect_sents function\n",
    "for doc in nlp.pipe(immune_df.head(100), disable=['parser','tagger','ner']): \n",
    "    matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the function to print the sentence of the matched term (span)\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    print('SPAN: {}'.format(span))\n",
    "\n",
    "    # span.sent provides the sentence that contains the span\n",
    "    print('SENT: {}'.format(span.sent))\n",
    "    print()\n",
    "\n",
    "\n",
    "# update the pattern to look for any noun preceeding the term 'fees'\n",
    "pattern = [{'POS': 'NOUN', 'OP': '+'},{'LOWER':'disease'}]\n",
    "matcher = Matcher(nlp.vocab)  # reinstantiate the matcher to remove previous patterns\n",
    "matcher.add('disease', collect_sents, pattern)\n",
    "\n",
    "for doc in nlp.pipe(immune_df.head(100), disable=['parser','ner']): # enable pos tagger\n",
    "    matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the function to collect sentences\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "        \n",
    "    # update matched data collections\n",
    "    matched_sents.append(span.sent)\n",
    "\n",
    "\n",
    "matched_sents = []  # container for sentences\n",
    "pattern = [{'POS': 'NOUN', 'OP': '+'},{'LOWER':'disease'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('disease', collect_sents, pattern)\n",
    "\n",
    "for doc in nlp.pipe(immune_df.head(100), disable=['ner']): # enable parser to collect sents\n",
    "    matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review matches\n",
    "set(matched_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the function to count matches using defaultdict\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]\n",
    "    \n",
    "    # update matched data collections\n",
    "    ent_count[span.text] += 1  # defaultdict keys must use span.text not span!\n",
    "\n",
    "\n",
    "ent_count = defaultdict(int)\n",
    "pattern = [{'LOWER':'disease'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('disease', collect_sents, pattern)\n",
    "\n",
    "for doc in nlp.pipe(immune_df.head(100), disable=['pos','ner']): # enable parser to collect sents\n",
    "    matcher(doc)\n",
    "\n",
    "ent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# update the pattern to look for a noun describing the term\n",
    "\n",
    "ent_count = defaultdict(int)\n",
    "\n",
    "# change OP to 1 to only get a single term to the left\n",
    "pattern = [{'POS': 'NOUN', 'OP': '1'},{'LOWER':'disease'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('disease', collect_sents, pattern)\n",
    "\n",
    "for doc in nlp.pipe(immune_df.head(1000), disable=['ner']): # enable parser to collect sents\n",
    "    matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ent_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multiple Patterns\n",
    "\n",
    "SpaCy matchers can use multiple patterns. Each pattern can be added to the Matcher individually with match.add and can use their own collect_sents function. Or use *patterns to add multiple patterns to the matcher at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sents = []\n",
    "ent_sents  = defaultdict(list)\n",
    "ent_count = defaultdict(int)\n",
    "\n",
    "# multiple patterns\n",
    "pattern = [[{'POS': 'NOUN', 'OP': '+'},{'LOWER': 'disease'}]\n",
    "           , [{'POS': 'NOUN', 'OP': '+'},{'LOWER': 'disorder'}]]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# *patterns to add multiple patterns with the same collect_sents function\n",
    "matcher.add('disease_disorder', collect_sents, *pattern)\n",
    "\n",
    "for doc in nlp.pipe(immune_df.head(500), disable=['ner']):\n",
    "    matches = matcher(doc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
